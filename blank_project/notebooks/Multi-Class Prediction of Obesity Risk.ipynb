{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68479,"databundleVersionId":10950255,"sourceType":"competition"},{"sourceId":7009925,"sourceType":"datasetVersion","datasetId":4030196}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Created by yunsuxiaozi\n\n#### This notebook is study from <a href=\"https://www.kaggle.com/code/ksevta/ps4e2-xgb-lgbm-0-92#Feature-Engineering-&-Processing\">PS4E2: XGB / LGBM | 0.92+</a>. If you think my notebook is useful,please upvote for me.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"#necessary\nimport pandas as pd#导入csv文件的库\nimport numpy as np#进行矩阵运算的库\nfrom sklearn.feature_extraction.text import TfidfVectorizer#导入tf-idf模型\nfrom sklearn.decomposition import TruncatedSVD#基于奇异值分解的降维方法:截断SVD\n#设置随机种子,保证模型可以复现\nimport random\nimport warnings#避免一些可以忽略的报错\nwarnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"#config\nclass Config():\n    origin_path=\"/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv\"\n    train_path='/kaggle/input/playground-series-s4e2/train.csv'\n    test_path='/kaggle/input/playground-series-s4e2/test.csv'\n    submission_path='/kaggle/input/playground-series-s4e2/sample_submission.csv'\n    seed=2024\n    num_folds=10\n    TARGET_NAME ='NObeyesdad'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## preprocessor","metadata":{}},{"cell_type":"code","source":"#一个处理数据的通用类,用于打playground比赛\nclass Preprocessor:#数据预处理的一个类\n    def __init__(self,train_path=None,origin_path=None,test_path=None,TARGET_NAME ='target',seed=2024):\n        self.seed=seed\n        #设置随机种子,保证模型可以复现\n        np.random.seed(self.seed)\n        random.seed(self.seed)\n        #需要预测的目标列\n        self.TARGET_NAME=TARGET_NAME \n        \n        if train_path!=None:#一般都有训练数据的路径\n            self.train_df=pd.read_csv(train_path)\n            self.train_df.drop(['id'],axis=1,inplace=True)\n        else:#没有训练数据的路径就报错\n            assert 0#assert 0==1?\n        \n        if origin_path!=None:#如果打算使用原始数据,没有原始数据也无所谓\n            origin_df=pd.read_csv(origin_path)\n            print(f\"len(origin_df):{len(origin_df)}\")\n            #如果有原始数据,则把训练数据和原始数据拼接\n            self.train_df=pd.concat((self.train_df,origin_df),axis=0)\n            print(f\"len(self.train_df):{len(self.train_df)}\")\n        self.train_df.fillna(method=\"ffill\",inplace=True)\n        self.train_df=self.train_df.drop_duplicates()\n        print(f\"len(self.train_df):{len(self.train_df)}\")\n            \n        if test_path!=None:#一般都有训练数据的路径\n            self.test_df=pd.read_csv(test_path)\n            self.test_df.drop(['id'],axis=1,inplace=True)\n            self.test_df.fillna(method=\"ffill\",inplace=True)#用上一个数据来填充\n        else:#没有训练数据的路径就报错\n            assert 0#assert 0==1?\n        #浮点数类型的连续型变量\n        self.num_cols=[]\n        #类别型变量,一般类别较少\n        self.cat_cols=[]\n        #一般就是像姓名那样不是类别型的字符串了\n        self.str_cols=[]\n        for col in self.train_df.columns:\n            if (self.train_df[col].dtype!='float') and (col!=self.TARGET_NAME) and (self.train_df[col].nunique()<50):\n                self.cat_cols.append(col)\n            elif (self.train_df[col].dtype=='float') and (col!=self.TARGET_NAME):\n                self.num_cols.append(col)\n            elif (col!=self.TARGET_NAME):#一般就是普通的字符串了\n                self.str_cols.append(col)\n        print(f\"num_cols:{self.num_cols}.\\ncat_cols:{self.cat_cols}.\\nstr_cols:{self.str_cols} \")\n        \n    #计算两组变量的皮尔逊相关系数\n    def pearson_corr(self,x1,x2):\n        #x1,x2 np.array\n        eps=1e-15\n        mean_x1=np.mean(x1)\n        mean_x2=np.mean(x2)\n        std_x1=np.std(x1)\n        std_x2=np.std(x2)\n        pearson=np.mean((x1-mean_x1)*(x2-mean_x2))/(std_x1*std_x2+eps)\n        return pearson\n    \n    #用于进行探索性数据分析的函数.\n    def EDA(self,):\n        print(\"num_cols VS cat_cols:\")\n        for num_col in self.num_cols:\n            for cat_col in self.cat_cols:\n                unique_value=self.train_df[cat_col].unique()\n                for value in unique_value:\n                    tmp_df=self.train_df[self.train_df[cat_col]==value]\n                    print(f\"{cat_col}=={value}:mean_{num_col}=={tmp_df[num_col].mean()}\")\n                print(\"-\"*50)\n            print()\n        print(\"num_cols VS num_cols:\")\n        for i in range(len(self.num_cols)):\n            for j in range(i+1,len(self.num_cols)):\n                x1=self.train_df[self.num_cols[i]].values\n                x2=self.train_df[self.num_cols[j]].values\n                if abs(self.pearson_corr(x1,x2))>0.9:\n                    print(f\"{self.num_cols[i]} and {self.num_cols[j]} have strong linear correlation.\")\n                    \n        #如果是分类任务\n        if self.train_df[self.TARGET_NAME].nunique()<50:\n            print(\"TARGET VS cat_cols:\")\n            unique_target=self.train_df[self.TARGET_NAME].unique()\n            for target in unique_target:\n                for cat_col in self.cat_cols:\n                    unique_value=self.train_df[cat_col].unique()\n                    for value in unique_value:\n                        #当cat_col==value时,target有没有唯一值\n                        tmp_df=self.train_df[self.train_df[cat_col]==value]\n                        #如果target基本只有它一个值\n                        if len(tmp_df[tmp_df[self.TARGET_NAME]==target])>0.99*len(tmp_df):\n                            print(f\"when {cat_col}={value},{self.TARGET_NAME}={target}\")\n                            \n                        tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]\n                        if len(tmp_df[tmp_df[cat_col]==value])>0.99*len(tmp_df):\n                            print(f\"when {self.TARGET_NAME}={target},{cat_col}={value}\")   \n            print(\"-\"*50)\n            print(\"TARGET VS num_cols:\")\n            for num_col in self.num_cols:\n                for target in unique_target:\n                    tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]\n                    print(f\"when {self.TARGET_NAME}={target},mean_{num_col}={tmp_df[num_col].mean()}\")  \n                print(\"-\"*50)      \n        else:#如果是回归任务的话,那肯定是数值类型的target\n            print(\"TARGET VS cat_cols:\")\n            for target in unique_target:\n                for cat_col in self.cat_cols:\n                    unique_value=self.train_df[cat_col].unique()\n                    for value in unique_value:\n                        tmp_df=self.train_df[self.train_df[cat_col]==value]\n                        print(f\"when {cat_col}={value},mean_{self.TARGET_NAME}={tmp_df[self.TARGET_NAME].mean()}\")  \n                print(\"-\"*50)   \n            \n                \n    \n    #对训练和测试数据某列字符串列构造tf-idf特征,取n个特征,最后降维为p个特征\n    def tf_idf(self,train, test, column,n,p):\n        print(f\"tf-idf done with {column}\")\n        vectorizer=TfidfVectorizer(max_features=n)#创建tf-idf模型,取出最重要的n个特征\n        #对训练数据的列进行fit,得到训练数据和测试数据的向量.\n        vectors_train=vectorizer.fit_transform(train[column])\n        vectors_test=vectorizer.transform(test[column])\n\n        svd=TruncatedSVD(p)#创建截断SVD,降维到P维\n        x_pca_train=svd.fit_transform(vectors_train)#对训练向量和测试向量进行降维\n        x_pca_test=svd.transform(vectors_test)\n        tfidf_df_train=pd.DataFrame(x_pca_train)#转成表格型数据\n        print(len(tfidf_df_train))\n        tfidf_df_test=pd.DataFrame(x_pca_test)\n        \n        #对列名进行调整 ‘Surname_tfidf_{idx}’\n        cols=[(column+\"_tfidf_\"+str(f)) for f in tfidf_df_train.columns]\n        tfidf_df_train.columns=cols\n        tfidf_df_test.columns=cols\n        #按列拼接在一起.\n        train=pd.concat([train,tfidf_df_train], axis=1)\n        test=pd.concat([test,tfidf_df_test], axis=1)\n        train.drop([column],axis=1,inplace=True)\n        test.drop([column],axis=1,inplace=True)\n        return train, test\n    #对类别型字符串进行独热编码\n    def one_hot_encoder(self,total_df):\n        print(f\"one hot encoder with {self.cat_cols}\")\n        for col in self.cat_cols:\n            if total_df[col].nunique()==2:#如果类别数量等于2的话,有一列onehot就行了.\n                value=total_df[col].values[0]\n                total_df[col+\"_\"+str(value)]=(total_df[col]==value)\n                total_df.drop([col],axis=1,inplace=True)\n            else:#如果类别数量很多的话,一个一个来\n                values=total_df[col].unique()\n                for value in values:\n                    total_df[col+\"_\"+str(value)]=(total_df[col]==value)\n                total_df.drop([col],axis=1,inplace=True)\n        return total_df          \n    #这个可能每次比赛都要具体情况具体分析.\n    def make_feats(self):\n        total_df=pd.concat((self.train_df,self.test_df),axis=0)\n        print(f\"len(total_df):{len(total_df)}\")\n        total_df=self.one_hot_encoder(total_df)\n        \n#         #增加特征BMI,将部分特征保留合适的小数位数\n#         total_df['Age']=round(total_df['Age'],2)\n#         total_df['Height']=round(total_df['Height'],2)\n#         total_df['Weight']=round(total_df['Weight'],2)\n#         total_df['BMI']=total_df['Weight']/(total_df['Height']**2)\n#         cols_to_round = ['FCVC',\"NCP\",\"CH2O\",\"FAF\",\"TUE\"]\n#         for col in cols_to_round:\n#             total_df[col] = round(total_df[col])\n#             total_df[col] = total_df[col].astype('int')        \n        \n        \n        print(\"-\"*50)\n        train_feats=total_df[:len(self.train_df)].reset_index(drop=True)\n        test_feats=total_df[len(self.train_df):].reset_index(drop=True)\n        print(len(train_feats))\n        for col in self.str_cols:\n            train_feats,test_feats=self.tf_idf(train_feats, test_feats, column=col,n=int(0.5*train_feats[col].nunique()),p=5)\n            print(f\"len(train_feats):{len(train_feats)}\")\n        print(\"-\"*50)\n        print(f\"Done!! total_features:{len(train_feats.keys().values)-1}\")\n        return train_feats,test_feats\n\n    \n    def submission(self,submission_path=None,test_pred=None):\n        if submission_path!=None:#一般都有训练数据的路径\n            self.submission_df=pd.read_csv(submission_path)\n        else:#没有提交文件的路径就报错\n            assert 0#assert 0==1?\n        self.submission_df[self.TARGET_NAME]=test_pred\n        self.submission_df.to_csv(\"submission.csv\",index=None)\n        return self.submission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature engineer","metadata":{}},{"cell_type":"code","source":"print(\"Import data.\")\npreprocessor=Preprocessor(Config.train_path,Config.origin_path,Config.test_path,Config.TARGET_NAME,Config.seed)\nprint(\"-\"*50)\nprint(\"EDA!!!\")\npreprocessor.EDA()\nprint(\"-\"*50)\nprint(\"make features.\")\ntrain_feats,test_feats=preprocessor.make_feats()\nprint(\"-\"*50)\nprint(\"labels to idx\")\nlabels=['Insufficient_Weight','Normal_Weight', 'Overweight_Level_I','Overweight_Level_II','Obesity_Type_I','Obesity_Type_II', 'Obesity_Type_III'] \nlabels2idx={}\nidx2labels={}\nfor idx in range(len(labels)): \n    labels2idx[labels[idx]]=idx \n    idx2labels[idx]=labels[idx]\ntrain_feats['NObeyesdad']=train_feats['NObeyesdad'].apply(lambda x: labels2idx[x])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"#model\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#KFold是直接分成k折,StratifiedKFold还要考虑每种类别的占比\nfrom sklearn.model_selection import StratifiedKFold\n\ndef fit_and_predict(model):\n    X=train_feats.drop([Config.TARGET_NAME],axis=1).copy()\n    y=train_feats[Config.TARGET_NAME].copy()\n    test_X=test_feats.drop([Config.TARGET_NAME],axis=1).copy()\n    oof_pred_pro=np.zeros((len(X),np.max(y)+1))\n    test_pred_pro=np.zeros((Config.num_folds,len(test_X),np.max(y)+1))\n    #10折交叉验证\n    skf = StratifiedKFold(n_splits=Config.num_folds,random_state=Config.seed, shuffle=True)\n\n    for fold, (train_index, valid_index) in (enumerate(skf.split(X, y.astype(str)))):\n        print(f\"fold:{fold}\")\n\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        model.fit(X_train,y_train)\n        \n        oof_pred_pro[valid_index]=model.predict_proba(X_valid)\n        test_pred_pro[fold]=model.predict_proba(test_X)\n    \n    return oof_pred_pro,test_pred_pro\n\n#选择的参数都来自这个notebook https://www.kaggle.com/code/ksevta/ps4e2-xgb-lgbm-0-92/notebook\nlgb_params={\n    \"objective\": \"multiclass\",          # Objective function for the model\n    \"metric\": \"multi_logloss\",          # Evaluation metric\n    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n    \"random_state\": Config.seed,       # Random state for reproducibility\n    \"num_class\": 7,                     # Number of classes in the dataset\n    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting\n    'n_estimators': 500,                # Number of boosting iterations\n    'lambda_l1': 0.009667446568254372,  # L1 regularization term\n    'lambda_l2': 0.04018641437301800,   # L2 regularization term\n    'max_depth': 10,                    # Maximum depth of the trees\n    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree\n    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration\n    'min_child_samples': 26             # Minimum number of data needed in a leaf\n}\n\nxgb_params = {'grow_policy': 'depthwise', 'n_estimators': 982, \n               'learning_rate': 0.050053726931263504, 'gamma': 0.5354391952653927, \n               'subsample': 0.7060590452456204, 'colsample_bytree': 0.37939433412123275, \n               'max_depth': 23, 'min_child_weight': 21, 'reg_lambda': 9.150224029846654e-08,\n               'reg_alpha': 5.671063656994295e-08,\n               'booster':'gbtree','objective':'multi:softmax',\"verbosity\":0\n              }\ncat_params={'learning_rate': 0.13762007048684638, 'depth': 5, \n          'l2_leaf_reg': 5.285199432056192, 'bagging_temperature': 0.6029582154263095,\n         'random_seed': Config.seed,'verbose': False,'iterations':1000}\n\nprint(\"random forest model\")\nrf_oof_pred_pro,rf_test_pred_pro=fit_and_predict(model=RandomForestClassifier(random_state=Config.seed))\nprint(\"lgb model\")\nlgb_oof_pred_pro,lgb_test_pred_pro=fit_and_predict(model= LGBMClassifier(**lgb_params,verbose=-1))\nprint(\"xgb model\")\nxgb_oof_pred_pro,xgb_test_pred_pro=fit_and_predict(model= XGBClassifier(**xgb_params,seed=Config.seed))\nprint(\"cat model\")\ncat_oof_pred_pro,cat_test_pred_pro=fit_and_predict(model= CatBoostClassifier(**cat_params))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blending","metadata":{}},{"cell_type":"code","source":"#评估指标是log_loss\ndef log_loss(y_true,y_pred):\n    eps=10**(-15)\n    y_true=np.clip(y_true,eps,1-eps)\n    y_pred=np.clip(y_pred,eps,1-eps)\n    return -np.mean(np.sum(y_true*np.log(y_pred),axis=-1))\ndef accuracy(y_true,y_pred):\n    return np.mean(y_true==y_pred)\noof_target=train_feats[Config.TARGET_NAME].values\noof_one_hot=np.eye(np.max(oof_target)+1)[oof_target]\nstep=100\nbest_w1=1\nbest_w2=1\nbest_w3=1\nbest_accuracy=0\nbest_log_loss=10\nfor w1 in range(1,step-1,1):\n    for w2 in range(1,step-w1-1,1):\n        for w3 in range(1,step-w1-w2-1,1):\n            blend_oof_pred_pro=(w1*rf_oof_pred_pro+w2*lgb_oof_pred_pro+w3*xgb_oof_pred_pro+(step-w1-w2-w3)*cat_oof_pred_pro)/step\n            current_log_loss=log_loss(oof_one_hot,blend_oof_pred_pro)\n            current_accuracy=accuracy(oof_target,np.argmax(blend_oof_pred_pro,axis=1))\n            if current_accuracy>best_accuracy:\n                best_w1=w1\n                best_w2=w2\n                best_w3=w3\n                best_accuracy=current_accuracy\n                best_log_loss=current_log_loss\n                print(f\"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}\")\n            elif (current_accuracy==best_accuracy) and current_log_loss<best_log_loss:\n                best_w1=w1\n                best_w2=w2\n                best_w3=w3\n                best_log_loss=current_log_loss\n                print(f\"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"blend_test_pred_pro=(best_w1*rf_test_pred_pro+best_w2*lgb_test_pred_pro+best_w3*xgb_test_pred_pro+(step-best_w1-best_w2-best_w3)*cat_test_pred_pro)/step\ntest_pred=np.argmax(blend_test_pred_pro.mean(axis=0),axis=1)\ntest_pred=[idx2labels[int(pred)] for pred in test_pred]\nprint(\"submission\")\nsubmission_df=preprocessor.submission(submission_path=Config.submission_path,test_pred=test_pred)\nsubmission_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}